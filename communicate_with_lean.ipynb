{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communicating with Lean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Communicating-with-Lean\" data-toc-modified-id=\"Communicating-with-Lean-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Communicating with Lean</a></span><ul class=\"toc-item\"><li><span><a href=\"#Lean-Server\" data-toc-modified-id=\"Lean-Server-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Lean Server</a></span></li><li><span><a href=\"#Testing-tactics-with-the-Lean-Server\" data-toc-modified-id=\"Testing-tactics-with-the-Lean-Server-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Testing tactics with the Lean Server</a></span><ul class=\"toc-item\"><li><span><a href=\"#Speed-and-robustness\" data-toc-modified-id=\"Speed-and-robustness-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Speed and robustness</a></span></li><li><span><a href=\"#Stepping-through-a-full-proof\" data-toc-modified-id=\"Stepping-through-a-full-proof-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Stepping through a full proof</a></span></li></ul></li><li><span><a href=\"#Creating-a-simple-Lean-solver-(with-breath-first-search)-in-Python\" data-toc-modified-id=\"Creating-a-simple-Lean-solver-(with-breath-first-search)-in-Python-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Creating a simple Lean solver (with breath first search) in Python</a></span></li><li><span><a href=\"#Custom-parsing-of-Lean-expressions\" data-toc-modified-id=\"Custom-parsing-of-Lean-expressions-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Custom parsing of Lean expressions</a></span></li><li><span><a href=\"#TODOs\" data-toc-modified-id=\"TODOs-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>TODOs</a></span></li><li><span><a href=\"#What-I-want-from-Lean-4\" data-toc-modified-id=\"What-I-want-from-Lean-4-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>What I want from Lean 4</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a hacky prototype showing how one can use Lean's server for communication between Lean and another program.  We have in mind implementing something like HOList for Lean. The main idea is that theorem proving is like a puzzle similar to the Rubik cube.  One has states and actions one can perform to move to other states.  This states are goals (or lists of goals) and the actions are tactics.  \n",
    "\n",
    "What we build here is an interface where one says to Lean \"apply this tactic to this goal\" and Lean says whether that tactic succeeds and if so what the new list of goals are.  It is written in a Python notebook, but the ideas transcend languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import json\n",
    "from pprint import pprint\n",
    "import time\n",
    "from datetime import datetime\n",
    "import collections\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lean Server\n",
    "This class starts up a Lean server and communicates with it.  There are a number of requests one can make to the Lean server, but we only need one or two.  To understand the Lean server, it is important to know it is not like a REPL, but more like the LSP.  It is designed to partial compile and comment on files as they are being written.  The use here is more of a hack (and it is unfortunately quite a bit slow we will see).\n",
    "\n",
    "One communicates to Lean by sending JSON requests and getting back responses.  We only need one or two request types for our purposes.  The first request we need is a \"sync\" request.  It sends a \"file\" to Lean to be synced.  (The file doesn't actually need to exist.)  The second request, an \"info\" request, asks Lean to provide more information about particular places in the code.  Unfortunately, the responses to the requests don't actually provide a lot of direct answers.  Especially for the \"sync\" request, one has to wait for an \"all_messages\" request to provide answers.  (And even then, it is difficult to determine that the response you got is up-to-date with the current Lean file.  We've employed a lot of tricks to make sure we are getting the most up-to-date information from Lean.)  Our class will store any \"all_messages\" responses it sees into a queue for latter processing.\n",
    "\n",
    "Ultimately, the \"info\" requests turned out not to be a reliable as we hoped, so we just use the \"sync\" request and only use the \"info\" request to prevent blocking of the channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LeanServer:\n",
    "    \"\"\"\n",
    "    Open up a Lean server and communicate with it.  \n",
    "    \n",
    "    Right now it only has support for the \"sync\" and \"info\" requests (and their)\n",
    "    corresponding responses.  It does however, store all \"all_messages\" response\n",
    "    into a queue for later processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cntr = -1\n",
    "        self.cntr2 = -1\n",
    "        self.all_messages = []\n",
    "        self.all_messages_time = -1\n",
    "        self.current_tasks = []\n",
    "        self.current_tasks_time = -1\n",
    "        self.proc = subprocess.Popen(['lean', '--server'], \n",
    "                    universal_newlines=True, \n",
    "                    stdin=subprocess.PIPE, # pipe STDIN and STDOUT to send and receive messages\n",
    "                    stdout=subprocess.PIPE, \n",
    "                    #stderr=subprocess.PIPE\n",
    "                )\n",
    "    \n",
    "    # make into a context manager so that it closes lean server automatically\n",
    "    def __enter__(self):\n",
    "        self.proc.__enter__()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.proc.__exit__(type, value, traceback)\n",
    "    \n",
    "    def seq_num(self):\n",
    "        self.cntr += 1\n",
    "        return self.cntr\n",
    "    \n",
    "    def send_request(self, request, expected_response, verbose=False, sleep=0.0):\n",
    "        seq_num = self.seq_num()\n",
    "        request1 = request.copy()\n",
    "        request1['seq_num'] = seq_num\n",
    "        j = json.dumps(request1)\n",
    "        \n",
    "        # TODO: Use logging instead\n",
    "        if verbose:\n",
    "            print()\n",
    "            print(\"=>:\", datetime.now().strftime('%H:%M:%S.%f'))\n",
    "            pprint(j)\n",
    "            print()\n",
    "        \n",
    "        # send\n",
    "        print(j, file=self.proc.stdin, flush=True)\n",
    "        \n",
    "        # wait for response\n",
    "        time.sleep(sleep)\n",
    "        while True:\n",
    "            self.cntr2 += 1\n",
    "            raw_output = self.proc.stdout.readline()\n",
    "            output = json.loads(raw_output)\n",
    "            if verbose:\n",
    "                print(\"<=:\", datetime.now().strftime('%H:%M:%S.%f'))\n",
    "                pprint(output)\n",
    "                print()\n",
    "            if 'response' in output:\n",
    "                if output['response'] == expected_response and output['seq_num'] == seq_num:\n",
    "                    return output\n",
    "                # TODO: Handle error differently.  It means the JSON is bad\n",
    "                elif output['response'] == 'error':\n",
    "                    return output\n",
    "                # record messages since they may point to errors in the lean code\n",
    "                elif output['response'] == 'all_messages':\n",
    "                    self.all_messages = output['msgs']\n",
    "                    self.all_messages_time = self.cntr2\n",
    "                # record tasks to know when Lean has stopped processing file\n",
    "                elif output['response'] == 'current_tasks':\n",
    "                    self.current_tasks = output['tasks']\n",
    "                    self.current_tasks_time = self.cntr2\n",
    "    \n",
    "    def send_sync_request(self, file_name, content, verbose=False):\n",
    "        request = {\n",
    "            'command':'sync',\n",
    "            'file_name': file_name,\n",
    "            \"content\": content,\n",
    "        }\n",
    "        \n",
    "        return self.send_request(request, expected_response='ok', verbose=verbose, sleep=0.0)\n",
    "    \n",
    "    def send_info_request(self, file_name, line, column, verbose=False):\n",
    "        request = {            \n",
    "            'command':'info',\n",
    "            'file_name': file_name,\n",
    "            'column': column,\n",
    "            'line': line\n",
    "        }\n",
    "        \n",
    "        return self.send_request(request, expected_response='ok', verbose=verbose, sleep=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'file invalidated', 'response': 'ok', 'seq_num': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'response': 'ok', 'seq_num': 1}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'caption': '',\n",
       "  'file_name': '/tmp/fake_lean_file.lean',\n",
       "  'pos_col': 0,\n",
       "  'pos_line': 1,\n",
       "  'severity': 'warning',\n",
       "  'text': \"declaration '[anonymous]' uses sorry\"}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sync file (the \"file\" doesn't need to exist)\n",
    "with LeanServer() as lean_server:\n",
    "    response1 = lean_server.send_sync_request(\n",
    "        file_name='/tmp/fake_lean_file.lean', \n",
    "        content='example (p: Prop) : p = p := begin sorry end')\n",
    "    time.sleep(1)\n",
    "    response2 = lean_server.send_info_request(\n",
    "        file_name='/tmp/fake_lean_file.lean', \n",
    "        line=1, \n",
    "        column=0)\n",
    "    \n",
    "    # the responses themselves aren't always useful\n",
    "    display(response1)\n",
    "    display(response2)\n",
    "    \n",
    "    # the \"all_messages\" responses are more useful\n",
    "    display(lean_server.all_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In much of the code, use the optional parameter `verbose=True` to see the message traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=>: 17:48:15.674784\n",
      "('{\"file_name\": \"/tmp/fake_lean_file.lean\", \"content\": \"example (p: Prop) : p '\n",
      " '= p := begin sorry end\", \"command\": \"sync\", \"seq_num\": 0}')\n",
      "\n",
      "<=: 17:48:15.951209\n",
      "{'is_running': False, 'response': 'current_tasks', 'tasks': []}\n",
      "\n",
      "<=: 17:48:16.161686\n",
      "{'is_running': False, 'response': 'current_tasks', 'tasks': []}\n",
      "\n",
      "<=: 17:48:16.368470\n",
      "{'is_running': False, 'response': 'current_tasks', 'tasks': []}\n",
      "\n",
      "<=: 17:48:16.596187\n",
      "{'is_running': False, 'response': 'current_tasks', 'tasks': []}\n",
      "\n",
      "<=: 17:48:16.629714\n",
      "{'message': 'file invalidated', 'response': 'ok', 'seq_num': 0}\n",
      "\n",
      "\n",
      "=>: 17:48:16.629889\n",
      "('{\"file_name\": \"/tmp/fake_lean_file.lean\", \"command\": \"info\", \"seq_num\": 1, '\n",
      " '\"line\": 1, \"column\": 0}')\n",
      "\n",
      "<=: 17:48:16.630577\n",
      "{'response': 'ok', 'seq_num': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sync file (the \"file\" doesn't need to exist)\n",
    "with LeanServer() as lean_server:\n",
    "    lean_server.send_sync_request(\n",
    "        file_name='/tmp/fake_lean_file.lean', \n",
    "        content='example (p: Prop) : p = p := begin sorry end',\n",
    "        verbose=True)\n",
    "    lean_server.send_info_request(\n",
    "        file_name='/tmp/fake_lean_file.lean', \n",
    "        line=1,\n",
    "        column=0,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing tactics with the Lean Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, tactic-based theorem proving is like a puzzle.  You go from state to state exploring.  What we need is a way to test if we can use a tactic on a goal and what the next goals are.  This is what the LeanTacticTestingInterface does.  (Note, it is much faster to ask the Lean server about multiple goal-tactic pairs instead of just one at a time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For storing information about a Lean goal\n",
    "LeanGoal = collections.namedtuple('LeanGoal', ['assumptions', 'conclusion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LeanTacticTestingInterface:\n",
    "    \"\"\"\n",
    "    Interface to test a tactic on a goal (or multiple tactics on multiple goals)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.lean_server = LeanServer()\n",
    "        \n",
    "        self.msg_cntr = 0\n",
    "        self.file_name = 'dummy.lean'   # the name of the \"file\" being checked (not a real file)\n",
    "        self.dummy_file = 'dummy.lean'  # a file always kept empty \n",
    "        self.eval_line_pos = None       # used for checking that a file is complete\n",
    "        \n",
    "        # set up a dummy file for later use\n",
    "        self.lean_server.send_sync_request(file_name=self.dummy_file, content=\"\")\n",
    "        \n",
    "        # one can implement their own custom state parser.  This is the default.\n",
    "        self.custom_parser = r\"\"\"\n",
    "meta def custom_parse_state : tactic string :=\n",
    "do return \"\" -- make this more interesting\n",
    "\"\"\"\n",
    "    \n",
    "     # make into a context manager so that it closes lean server automatically\n",
    "    def __enter__(self):\n",
    "        self.lean_server.__enter__()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.lean_server.__exit__(type, value, traceback)\n",
    "        \n",
    "    def parse_state(self, goal_state):\n",
    "        \"\"\"\n",
    "        Lightly parses the Lean pretty-printed goal state string, returning a LeanGoal object.\n",
    "        \"\"\"\n",
    "        goals = []\n",
    "        assumptions = []\n",
    "        lines = goal_state.split('\\n')\n",
    "        for l in lines:\n",
    "            if \"⊢ \" in l:\n",
    "                conclusion = l[2:]\n",
    "                goal = LeanGoal(assumptions, l[2:])\n",
    "                goals.append(goal)\n",
    "                assumptions = []\n",
    "            elif \" : \" in l:\n",
    "                if l[-1] == \",\":\n",
    "                    assumptions.append(l[:-1])\n",
    "                else:\n",
    "                    assumptions.append(l)\n",
    "\n",
    "        return goals\n",
    "    \n",
    "    def check_content(self, content, verbose):\n",
    "        \"\"\"\n",
    "        Have Lean check the contents of a Lean \"file\". \n",
    "        \"\"\"\n",
    "        self.msg_cntr += 1\n",
    "        \n",
    "        # empty out the previous file for good measure\n",
    "        self.lean_server.send_sync_request(file_name=self.file_name, content=\"\", verbose=verbose)\n",
    "        \n",
    "        # use a few different file names to help avoid old data in the messages\n",
    "        self.file_name = 'fake_lean_file{}.lean'.format(self.msg_cntr % 10)\n",
    "        \n",
    "        # put an eval at the end to make sure the response is not from an old file\n",
    "        extended_content = content + \"\\n#eval \" + str(self.msg_cntr)\n",
    "        self.eval_line_pos = extended_content.count(\"\\n\") + 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"-- {} --------------\".format(file_name))\n",
    "            for i, l in enumerate(extended_content.split('\\n')):\n",
    "                print(\"{:3}   {}\".format(i+1, l))\n",
    "            print(\"-------------------------\")\n",
    "            \n",
    "        # send the file\n",
    "        self.lean_server.send_sync_request(file_name=self.file_name, content=extended_content, verbose=verbose)\n",
    "        \n",
    "    def get_messages(self, verbose):\n",
    "        \"\"\"\n",
    "        Get messages for the current content being checked.\n",
    "        Return all the messages for that \"file\"\n",
    "        \n",
    "        This is really really hard to make robust.  How do I know Lean has finished\n",
    "        or worst that Lean isn't still providing information from an old\n",
    "        version of the file?\n",
    "        \"\"\"\n",
    "        \n",
    "        # wait for tasks to stop running\n",
    "        tasks_stopped_time = None\n",
    "        for _ in range(10000):  # lazy time-out mechanism\n",
    "            \n",
    "            # keep sending requests to a dummy file to collect more \"all_messages\" responses\n",
    "            self.lean_server.send_info_request(file_name=self.dummy_file, line=1, column=0, verbose=verbose)\n",
    "            \n",
    "            # check that tasks are complete for file_name\n",
    "            task_running = False\n",
    "            for task in self.lean_server.current_tasks:\n",
    "                if task['file_name'] == self.file_name:\n",
    "                    task_running = True\n",
    "                    break\n",
    "            if not task_running:\n",
    "                tasks_stopped_time = self.lean_server.current_tasks_time\n",
    "                break\n",
    "        else:\n",
    "            raise TimeoutError\n",
    "            \n",
    "                \n",
    "        # check that messages are complete by looking at eval at the end\n",
    "        for _ in range(10000):  # lazy time-out mechanism\n",
    "            \n",
    "            # keep sending requests to a dummy file to collect more \"all_messages\" responses\n",
    "            self.lean_server.send_info_request(self.dummy_file, line=1, column=0, verbose=verbose)\n",
    "            \n",
    "            # wait until messages are after current tasks\n",
    "            #if self.lean_server.all_messages_time < tasks_stopped_time:\n",
    "            #    continue\n",
    "                \n",
    "            # filter the messages to those for this \"file\" \n",
    "            # and also check for the final the eval statement\n",
    "            filtered_messages = []\n",
    "            messages_are_complete = False\n",
    "            for msg in self.lean_server.all_messages:\n",
    "                if msg['file_name'] == self.file_name:\n",
    "                    if msg['pos_line'] < self.eval_line_pos:\n",
    "                        filtered_messages.append(msg)\n",
    "                    elif msg['pos_line'] == self.eval_line_pos and msg['caption'] == \"eval result\":\n",
    "                        if msg['text'] == str(self.msg_cntr):\n",
    "                            messages_are_complete = True\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "            # clear messages\n",
    "            self.lean_server.all_messages = []\n",
    "            \n",
    "            if messages_are_complete:\n",
    "                return filtered_messages\n",
    "            \n",
    "        raise TimeoutError\n",
    "    \n",
    "    def preamble(self):\n",
    "        return self.custom_parser + r\"\"\"\n",
    "        \n",
    "meta def trace_custom_state : tactic unit :=\n",
    "do \n",
    " s ← custom_parse_state,\n",
    " tactic.trace s,\n",
    " return ()\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "    def example_block(self, goal, tactic):\n",
    "        block = \"\"\n",
    "        # line: preamble + 6 * n + 0\n",
    "        block += \"example \"\n",
    "        block += \" \".join(\"(\" + l + \")\" for l in goal.assumptions)\n",
    "        block += \" : \"\n",
    "        block += goal.conclusion + \" :=\\n\"\n",
    "        # line: preamble + 6 * n + 1\n",
    "        block += \"begin\\n\"  \n",
    "        # line: preamble + 6 * n + 2\n",
    "        block += tactic + \",\\n\"\n",
    "        # line: preamble + 6 * n + 3\n",
    "        block += 'trace_custom_state,\\n' # custom parsed goal state\n",
    "        # line: preamble + 6 * n + 4\n",
    "        block += 'trace_state,\\n'        # pretty printed goal state\n",
    "        # line: preamble + 6 * n + 5\n",
    "        block += \"end\\n\"\n",
    "\n",
    "        return block\n",
    "            \n",
    "    def apply_tactic_multiple(self, lean_goals, tactics, verbose=False):\n",
    "        assert len(lean_goals) == len(tactics)\n",
    "        \n",
    "        # build content\n",
    "        preamble = self.preamble()\n",
    "        \n",
    "        # this is the line (indexed from 1) that the first block will be on\n",
    "        preamble_length = preamble.count(\"\\n\") + 1\n",
    "        \n",
    "        content = preamble\n",
    "        for goal, tactic in zip(lean_goals, tactics):\n",
    "            content += self.example_block(goal, tactic)\n",
    "        \n",
    "        # start checking the file\n",
    "        self.check_content(content, verbose=verbose)\n",
    "            \n",
    "        # positions in the block\n",
    "        theorem_line = 0\n",
    "        tactic_line = 2\n",
    "        custom_state_line = 3\n",
    "        goals_line = 4\n",
    "        block_length = 6\n",
    "        \n",
    "        for _ in range(1000):  # lazy timeout method\n",
    "            # start checking the file\n",
    "            msgs = self.get_messages(verbose=verbose)\n",
    "        \n",
    "            # extract data from messages\n",
    "            results = [[None, None, [], []] for _ in lean_goals]\n",
    "            for msg in msgs:\n",
    "                block_line_pos = (msg['pos_line'] - preamble_length) % block_length\n",
    "                block_num = (msg['pos_line'] - preamble_length) // block_length\n",
    "\n",
    "                if msg['pos_line'] < preamble_length:\n",
    "                    continue\n",
    "                elif block_line_pos == goals_line and msg['severity'] == 'information':\n",
    "                    results[block_num][0] = msg['text']\n",
    "                elif block_line_pos == custom_state_line and msg['severity'] == 'information':\n",
    "                    results[block_num][1] = msg['text']\n",
    "                elif block_line_pos == tactic_line and msg['severity'] == 'error':\n",
    "                    results[block_num][2].append(msg['text'])\n",
    "                elif block_line_pos == theorem_line and msg['severity'] == 'error':\n",
    "                    results[block_num][3].append(msg['text'])\n",
    "\n",
    "            # parse data and check for completeness\n",
    "            final_results = []\n",
    "            complete = True\n",
    "            for i, (goal_state, custom_state, tactic_errors, theorem_errors) in enumerate(results):\n",
    "                if theorem_errors:\n",
    "                    result = ('theorem_error', theorem_errors, '')\n",
    "                elif tactic_errors:\n",
    "                    result = ('tactic_failure', tactic_errors, '')\n",
    "                elif goal_state is not None and custom_state is not None:\n",
    "                    result = ('success', self.parse_state(goal_state), custom_state.strip())\n",
    "                else:\n",
    "                    complete = False  # need to get messages again\n",
    "                    break\n",
    "\n",
    "                final_results.append(result)\n",
    "\n",
    "            if complete:\n",
    "                return final_results\n",
    "        \n",
    "        raise TimeoutError\n",
    "        \n",
    "    def apply_tactic(self, lean_goal, tactic, verbose=False):\n",
    "        \"\"\"\n",
    "        Apply tactic to a goal and see what you get.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.apply_tactic_multiple([lean_goal], [tactic], verbose=verbose)\n",
    "        return result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see if we apply `refl` to `p = p` we solve all the goals (hence the empty list returned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('success', [], '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with LeanTacticTestingInterface() as lean_interface:\n",
    "    goal = LeanGoal(['p q : Prop'],  'p = p')\n",
    "    display(lean_interface.apply_tactic(goal, 'refl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what is going on, we show the logs with `verbose=True`.  This also prints the file being sent to Lean.  \n",
    "\n",
    "The main idea is that we encode everything we want to check into a Lean file, send that via a \"sync\" request, and read off the response from the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('success', [], '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with LeanTacticTestingInterface() as lean_interface:\n",
    "    goal = LeanGoal(['p q : Prop'],  'p = p')\n",
    "    display(lean_interface.apply_tactic(goal, 'refl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('success', [], '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('success', [LeanGoal(assumptions=['p q : Prop'], conclusion='p = p')], '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('tactic_failure',\n",
       " ['done tactic failed, there are unsolved goals\\nstate:\\np q : Prop\\n⊢ p = p'],\n",
       " '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('tactic_failure',\n",
       " [\"unknown identifier 'abc123'\",\n",
       "  \"don't know how to synthesize placeholder\\ncontext:\\np q : Prop\\n⊢ Type ?\"],\n",
       " '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('success',\n",
       " [LeanGoal(assumptions=['m : ℕ'], conclusion='0 = 0'),\n",
       "  LeanGoal(assumptions=['m n_n : ℕ', 'n_ih : n_n = n_n'], conclusion='nat.succ n_n = nat.succ n_n')],\n",
       " '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('theorem_error',\n",
       " [\"unknown identifier 'abc'\",\n",
       "  \"unknown identifier 'xyz'\",\n",
       "  \"don't know how to synthesize placeholder\\ncontext:\\n⊢ Sort ?\"],\n",
       " '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('theorem_error',\n",
       " [\"unknown identifier 'abc'\",\n",
       "  \"unknown identifier 'xyz'\",\n",
       "  \"don't know how to synthesize placeholder\\ncontext:\\n⊢ Sort ?\"],\n",
       " '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lots of examples\n",
    "with LeanTacticTestingInterface() as lean_interface:\n",
    "    goal1 = LeanGoal(['p q : Prop'],  'p = p')\n",
    "    display(lean_interface.apply_tactic(goal1, 'refl'))\n",
    "    display(lean_interface.apply_tactic(goal1, 'skip'))\n",
    "    display(lean_interface.apply_tactic(goal1, 'done'))\n",
    "    display(lean_interface.apply_tactic(goal1, 'abc123'))\n",
    "    goal2 = LeanGoal(['n m : nat'],  'n = n')\n",
    "    display(lean_interface.apply_tactic(goal2, 'induction n'))\n",
    "    goal3 = LeanGoal([], 'abc = xyz')\n",
    "    display(lean_interface.apply_tactic(goal3, 'skip'))\n",
    "    display(lean_interface.apply_tactic(goal3, 'fgh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('success', [], ''),\n",
       " ('success', [LeanGoal(assumptions=['p q : Prop'], conclusion='p = p')], ''),\n",
       " ('tactic_failure',\n",
       "  ['done tactic failed, there are unsolved goals\\nstate:\\np q : Prop\\n⊢ p = p'],\n",
       "  ''),\n",
       " ('tactic_failure',\n",
       "  [\"unknown identifier 'abc123'\",\n",
       "   \"don't know how to synthesize placeholder\\ncontext:\\np q : Prop\\n⊢ Type ?\"],\n",
       "  ''),\n",
       " ('success',\n",
       "  [LeanGoal(assumptions=['m : ℕ'], conclusion='0 = 0'),\n",
       "   LeanGoal(assumptions=['m n_n : ℕ', 'n_ih : n_n = n_n'], conclusion='nat.succ n_n = nat.succ n_n')],\n",
       "  ''),\n",
       " ('theorem_error',\n",
       "  [\"unknown identifier 'abc'\",\n",
       "   \"unknown identifier 'xyz'\",\n",
       "   \"don't know how to synthesize placeholder\\ncontext:\\n⊢ Sort ?\"],\n",
       "  ''),\n",
       " ('theorem_error',\n",
       "  [\"unknown identifier 'abc'\",\n",
       "   \"unknown identifier 'xyz'\",\n",
       "   \"don't know how to synthesize placeholder\\ncontext:\\n⊢ Sort ?\"],\n",
       "  '')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# examples of executing in batch\n",
    "with LeanTacticTestingInterface() as lean_interface:\n",
    "    goal1 = LeanGoal(['p q : Prop'],  'p = p')\n",
    "    goal2 = LeanGoal(['n m : nat'],  'n = n')\n",
    "    goal3 = LeanGoal([], 'abc = xyz')\n",
    "    \n",
    "    goals = [goal1, goal1, goal1, goal1, goal2, goal3, goal3]\n",
    "    tactics = ['refl', 'skip', 'done', 'abc123', 'induction n', 'skip', 'fgh']\n",
    "    \n",
    "    display(lean_interface.apply_tactic_multiple(goals, tactics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed and robustness\n",
    "Unfortunately, this is quit slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Each call to the Lean interface takes on average 0.10969949722290039 seconds'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time it\n",
    "total_runs = 50\n",
    "total_time = 0\n",
    "goal = LeanGoal([],  'true')\n",
    "with LeanTacticTestingInterface() as lean_interface:\n",
    "    # give the system a chance to \"warm up\"\n",
    "    lean_interface.apply_tactic(goal, 'refl')\n",
    "    time.sleep(2)\n",
    "    \n",
    "    for i in range(total_runs):\n",
    "        a = time.time()\n",
    "        response = lean_interface.apply_tactic(goal, 'refl')\n",
    "        b = time.time()\n",
    "        total_time += b - a\n",
    "\"Each call to the Lean interface takes on average {} seconds\".format(total_time / total_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sending multiple calls to Lean speeds things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Each call to the Lean interface takes on average 0.33645894050598146 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Each goal/tactic pair (w/ batch size 32) takes on average 0.01051434189081192 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# time running multiple goal-tactics at a time \n",
    "total_runs = 50\n",
    "batch_size = 32\n",
    "total_time = 0\n",
    "goals = [LeanGoal([],  'true')] * batch_size\n",
    "tactics = ['refl'] * batch_size\n",
    "with LeanTacticTestingInterface() as lean_interface:\n",
    "    # give the system a chance to \"warm up\"\n",
    "    lean_interface.apply_tactic(LeanGoal([],  'true'), 'refl')\n",
    "    time.sleep(2)\n",
    "    \n",
    "    for i in range(total_runs):\n",
    "        a = time.time()\n",
    "        response = lean_interface.apply_tactic_multiple(goals, tactics)\n",
    "        b = time.time()\n",
    "        total_time += b - a\n",
    "display(\"Each call to the Lean interface takes on average {} seconds\".format(total_time / total_runs))\n",
    "display(\"Each goal/tactic pair (w/ batch size {}) takes on average {} seconds\".format(batch_size, total_time / total_runs / batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very difficult to make this process robust. If one is not careful, the results returned by `apply_tactic_multiple` could be for a previous call to the method.  There are ways to get around this, but if one is not careful they would significantly increase Lean's memory usage over time.  (For example, using a new file name for every \"sync\" request would be bad, since Lean caches the results of each file.)  This is a test that `apply_tactic_multiple` is robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Passed tests'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test robustness.  Make sure we are getting correct results back \n",
    "# and not results for a previous request.\n",
    "rounds = 100\n",
    "with LeanTacticTestingInterface() as lean_interface:\n",
    "    for r in range(rounds):\n",
    "        count = rounds - r\n",
    "        goals = [LeanGoal(['p_{}_{} : Prop'.format(r, i)],  'p_{}_{}'.format(r, i)) for i in range(count)]\n",
    "        \n",
    "        if r % 3 == 0:\n",
    "            tactics = ['skip' for _ in range(count)]\n",
    "            results = lean_interface.apply_tactic_multiple(goals, tactics)\n",
    "            assert len(results) == count\n",
    "            for i, (response, new_goals, _) in enumerate(results):\n",
    "                assert response == 'success'\n",
    "                assert new_goals[0].conclusion == 'p_{}_{}'.format(r, i)\n",
    "        else:\n",
    "            tactics = ['simp' for _ in range(count)]\n",
    "            results = lean_interface.apply_tactic_multiple(goals, tactics)\n",
    "            assert len(results) == count\n",
    "            for i, (response, _, _) in enumerate(results):\n",
    "                assert response == 'tactic_failure', response\n",
    "\"Passed tests\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping through a full proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('success',\n",
       " [LeanGoal(assumptions=['m : ℕ'], conclusion='m + 0 = 0 + m'),\n",
       "  LeanGoal(assumptions=['m n_n : ℕ', 'n_ih : m + n_n = n_n + m'], conclusion='m + nat.succ n_n = nat.succ n_n + m')],\n",
       " '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with LeanTacticTestingInterface() as lean_interface:\n",
    "    display(lean_interface.apply_tactic(LeanGoal(['m n : nat'], \"m + n = n + m\"), \"induction n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('success',\n",
       " [LeanGoal(assumptions=['n_n : ℕ', 'n_ih : 0 + n_n = n_n + 0'], conclusion='0 + nat.succ n_n = nat.succ n_n + 0'),\n",
       "  LeanGoal(assumptions=['n_n m_n : ℕ', 'm_ih : m_n + n_n = n_n + m_n → m_n + nat.succ n_n = nat.succ n_n + m_n', 'n_ih : nat.succ m_n + n_n = n_n + nat.succ m_n'], conclusion='nat.succ m_n + nat.succ n_n = nat.succ n_n + nat.succ m_n')],\n",
       " '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with LeanTacticTestingInterface() as lean_interface:\n",
    "    display(lean_interface.apply_tactic(\n",
    "        LeanGoal(['m n_n : ℕ','n_ih : m + n_n = n_n + m'], 'm + nat.succ n_n = nat.succ n_n + m'), \n",
    "        \"induction m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('success', [], '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with LeanTacticTestingInterface() as lean_interface:\n",
    "    display(lean_interface.apply_tactic(\n",
    "        LeanGoal(['n_n : ℕ','n_ih : 0 + n_n = n_n + 0'],\n",
    "         '0 + nat.succ n_n = nat.succ n_n + 0'), \n",
    "        \"simp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('success', [], '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with LeanTacticTestingInterface() as lean_interface:\n",
    "    display(lean_interface.apply_tactic(\n",
    "        LeanGoal(['n_n m_n : ℕ',\n",
    "         'm_ih : m_n + n_n = n_n + m_n → m_n + nat.succ n_n = nat.succ n_n + m_n',\n",
    "         'n_ih : nat.succ m_n + n_n = n_n + nat.succ m_n'],\n",
    "         'nat.succ m_n + nat.succ n_n = nat.succ n_n + nat.succ m_n'), \n",
    "        \"simp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple Lean solver (with breath first search) in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we put all of this together to create (a non-practical) BFS prover.  Some highlights:\n",
    "- Some tactics like `apply` take a parameter.  We search the local context for all possible things to put in this parameter.  Note, we do no type checking before sending it to Lean.  The user inputs the tactic as `apply {}` to signify that something needs to be filled in from the context.\n",
    "- Our setup assumes the tactic is only applied to the first goal.  This isn't true of all tactics, so we force this by \"treeifying\" our proof, that is adding brackets `{ ... }` where necessary.\n",
    "- Sending multiple calls to Lean at the same time makes it much faster.  This is the only difference between the \"fast\" version.\n",
    "- We provide a way to verify a proof in the end (sending the whole proof as if it was a single tactic), and also walking through the proof to get information (which might be useful for machine-learning-like training).\n",
    "- This is just a proof-of-concept and not very useful in itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def premises_from_goal(goal):\n",
    "    prems = []\n",
    "    for l in goal.assumptions:\n",
    "        a, _ = l.split(\":\")\n",
    "        for v in a.strip().split(\" \"):\n",
    "            prems.append(v)\n",
    "    return prems\n",
    "\n",
    "def tactic_applications(tactics, prems):\n",
    "    tacs = []\n",
    "    for t in tactics:\n",
    "        if \"{}\" in t:\n",
    "            for p in prems:\n",
    "                tacs.append(t.replace(\"{}\", p))\n",
    "        else:\n",
    "            tacs.append(t)\n",
    "    return tacs\n",
    "\n",
    "def treeify_proof(proof_list):\n",
    "    s = \"\"\n",
    "    goal_cnts = [1]  # open goal counts at each depth\n",
    "    for tactic, new_goal_cnt in proof_list:\n",
    "        while True:\n",
    "            # take goals\n",
    "            goal_cnt = goal_cnts.pop()\n",
    "\n",
    "            # if no goals at this depth, end block and take off stack again\n",
    "            if goal_cnt == 0:\n",
    "                s += \", }\"\n",
    "                continue\n",
    "            \n",
    "            if goal_cnt == 1:\n",
    "                if s: # don't put comma in front\n",
    "                    s += \", \"\n",
    "                \n",
    "            # if more than one goal, take one of them, start block and put rest back on stack\n",
    "            if goal_cnt > 1:\n",
    "                goal_cnts.append(goal_cnt - 1)\n",
    "                s += \", { \"\n",
    "            \n",
    "            break\n",
    "            \n",
    "        # apply tactic\n",
    "        s += tactic\n",
    "        \n",
    "        # put remaining goals on stack\n",
    "        goal_cnts.append(new_goal_cnt)\n",
    "            \n",
    "    return s \n",
    "\n",
    "class BreathFirstProofSearch(LeanTacticTestingInterface):\n",
    "        \n",
    "    def proof_search(self, lean_goal, tactics, verbose=False):\n",
    "        starting_goal = lean_goal\n",
    "        starting_goal_list = (starting_goal,)\n",
    "        empty_tactics = tuple()\n",
    "        goal_list_queue = collections.deque()\n",
    "        goal_list_queue.append((starting_goal_list, empty_tactics))\n",
    "\n",
    "        while goal_list_queue:\n",
    "            goal_list, ts = goal_list_queue.popleft()\n",
    "            goal = goal_list[0]\n",
    "            prems = premises_from_goal(goal)\n",
    "            all_tactics = tactic_applications(tactics, prems)\n",
    "            for t in all_tactics:\n",
    "                response, new_goals, _ = self.apply_tactic(goal, t, verbose=verbose)\n",
    "                if response == 'success':\n",
    "                    new_goal_list = tuple(new_goals) + goal_list[1:]\n",
    "                    new_ts = ts + ((t, len(new_goals)),)\n",
    "                    if not new_goal_list:\n",
    "                        return treeify_proof(new_ts)\n",
    "\n",
    "                    goal_list_queue.append((new_goal_list, new_ts))\n",
    "        return None\n",
    "    \n",
    "    def proof_search_fast(self, lean_goal, tactics, verbose=False):\n",
    "        starting_goal = lean_goal\n",
    "        starting_goal_list = (starting_goal,)\n",
    "        empty_tactics = tuple()\n",
    "        goal_list_queue = collections.deque()\n",
    "        goal_list_queue.append((starting_goal_list, empty_tactics))\n",
    "\n",
    "        while goal_list_queue:\n",
    "            goal_list, ts = goal_list_queue.popleft()\n",
    "            goal = goal_list[0]\n",
    "            prems = premises_from_goal(goal)\n",
    "            all_tactics = tactic_applications(tactics, prems)\n",
    "            \n",
    "            # faster to batch process all apply_tactic calls\n",
    "            results = self.apply_tactic_multiple([goal] * len(all_tactics), all_tactics, verbose=verbose)\n",
    "            for t, (response, new_goals, _) in zip(all_tactics, results):\n",
    "                if response == 'success':\n",
    "                    new_goal_list = tuple(new_goals) + goal_list[1:]\n",
    "                    new_ts = ts + ((t, len(new_goals)),)\n",
    "                    if not new_goal_list:\n",
    "                        return treeify_proof(new_ts)\n",
    "\n",
    "                    goal_list_queue.append((new_goal_list, new_ts))\n",
    "        return None\n",
    "    \n",
    "    def is_valid_proof(self, goal, proof, verbose=False):\n",
    "        t = proof.replace(\"\\n\", \"\")  # treat proof as a single tactic\n",
    "        response, goals, _ = self.apply_tactic(goal, t, verbose=verbose)\n",
    "        if response == 'success' and goals == []:\n",
    "            return True\n",
    "        return goals\n",
    "    \n",
    "    def walk_through_proof(self, lean_goal, proof):\n",
    "        ts = proof.replace(\" {\", \"\").replace(\"}, \", \"\").split(\", \")\n",
    "        goals = [lean_goal]\n",
    "        for t in ts:\n",
    "            goal = goals[0]\n",
    "            print(\" Primary Goal:  \", goal)\n",
    "            print(\" Tactic applied:\", t)\n",
    "            _, new_goals, _ = self.apply_tactic(goal, t)\n",
    "            goals = new_goals + goals[1:]\n",
    "            print(\" New Goals: \", new_goals)\n",
    "            print(\" All Goals: \", goals)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def example(goal, tactics, verbose=False):\n",
    "    with BreathFirstProofSearch() as bfs:\n",
    "        print(\"Goal:\", goal)\n",
    "        print(\"Tactics:\", tactics)\n",
    "        print()\n",
    "        start = time.time()\n",
    "        proof = bfs.proof_search_fast(goal, tactics, verbose=verbose)\n",
    "        end = time.time()\n",
    "        print(\"Time to proof:\", end-start, \"seconds\")\n",
    "        print(\"Proof:\", proof)\n",
    "        print()\n",
    "        print(\"Verified:\", bfs.is_valid_proof(goal, proof, verbose=verbose))\n",
    "        print()\n",
    "        print(\"Walk through proof:\")\n",
    "        bfs.walk_through_proof(goal, proof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal: LeanGoal(assumptions=['p q r : Prop'], conclusion='p ∧ q → q ∧ p')\n",
      "Tactics: ['apply {}', 'cases {}', 'intro', 'split', 'left', 'right']\n",
      "\n",
      "Time to proof: 2.4912068843841553 seconds\n",
      "Proof: intro, cases a, split, { apply a_right, }, apply a_left\n",
      "\n",
      "Verified: True\n",
      "\n",
      "Walk through proof:\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop'], conclusion='p ∧ q → q ∧ p')\n",
      " Tactic applied: intro\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q'], conclusion='q ∧ p')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q'], conclusion='q ∧ p')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q'], conclusion='q ∧ p')\n",
      " Tactic applied: cases a\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_left : p', 'a_right : q'], conclusion='q ∧ p')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_left : p', 'a_right : q'], conclusion='q ∧ p')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_left : p', 'a_right : q'], conclusion='q ∧ p')\n",
      " Tactic applied: split\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_left : p', 'a_right : q'], conclusion='q'), LeanGoal(assumptions=['p q r : Prop', 'a_left : p', 'a_right : q'], conclusion='p')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_left : p', 'a_right : q'], conclusion='q'), LeanGoal(assumptions=['p q r : Prop', 'a_left : p', 'a_right : q'], conclusion='p')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_left : p', 'a_right : q'], conclusion='q')\n",
      " Tactic applied: apply a_right\n",
      " New Goals:  []\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_left : p', 'a_right : q'], conclusion='p')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_left : p', 'a_right : q'], conclusion='p')\n",
      " Tactic applied: apply a_left\n",
      " New Goals:  []\n",
      " All Goals:  []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example(\n",
    "    goal = LeanGoal(['p q r : Prop'], 'p ∧ q → q ∧ p'), \n",
    "    tactics=['apply {}', 'cases {}', 'intro', 'split', 'left', 'right'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal: LeanGoal(assumptions=['p q r : Prop'], conclusion='(p -> (q -> r)) <-> (p /\\\\ q -> r)')\n",
      "Tactics: ['apply {}', 'cases {}', 'intro', 'split', 'left', 'right']\n",
      "\n",
      "Time to proof: 6.05386209487915 seconds\n",
      "Proof: split, { intro, intro, cases a_1, apply a, { apply a_1_left, }, apply a_1_right, }, intro, intro, intro, apply a, split, { apply a_1, }, apply a_2\n",
      "\n",
      "Verified: True\n",
      "\n",
      "Walk through proof:\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop'], conclusion='(p -> (q -> r)) <-> (p /\\\\ q -> r)')\n",
      " Tactic applied: split\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop'], conclusion='(p → q → r) → p ∧ q → r'), LeanGoal(assumptions=['p q r : Prop'], conclusion='(p ∧ q → r) → p → q → r')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop'], conclusion='(p → q → r) → p ∧ q → r'), LeanGoal(assumptions=['p q r : Prop'], conclusion='(p ∧ q → r) → p → q → r')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop'], conclusion='(p → q → r) → p ∧ q → r')\n",
      " Tactic applied: intro\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r'], conclusion='p ∧ q → r')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r'], conclusion='p ∧ q → r'), LeanGoal(assumptions=['p q r : Prop'], conclusion='(p ∧ q → r) → p → q → r')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r'], conclusion='p ∧ q → r')\n",
      " Tactic applied: intro\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r', 'a_1 : p ∧ q'], conclusion='r')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r', 'a_1 : p ∧ q'], conclusion='r'), LeanGoal(assumptions=['p q r : Prop'], conclusion='(p ∧ q → r) → p → q → r')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r', 'a_1 : p ∧ q'], conclusion='r')\n",
      " Tactic applied: cases a_1\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r', 'a_1_left : p', 'a_1_right : q'], conclusion='r')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r', 'a_1_left : p', 'a_1_right : q'], conclusion='r'), LeanGoal(assumptions=['p q r : Prop'], conclusion='(p ∧ q → r) → p → q → r')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r', 'a_1_left : p', 'a_1_right : q'], conclusion='r')\n",
      " Tactic applied: apply a\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r', 'a_1_left : p', 'a_1_right : q'], conclusion='p'), LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r', 'a_1_left : p', 'a_1_right : q'], conclusion='q')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r', 'a_1_left : p', 'a_1_right : q'], conclusion='p'), LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r', 'a_1_left : p', 'a_1_right : q'], conclusion='q'), LeanGoal(assumptions=['p q r : Prop'], conclusion='(p ∧ q → r) → p → q → r')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r', 'a_1_left : p', 'a_1_right : q'], conclusion='p')\n",
      " Tactic applied: apply a_1_left\n",
      " New Goals:  []\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r', 'a_1_left : p', 'a_1_right : q'], conclusion='q'), LeanGoal(assumptions=['p q r : Prop'], conclusion='(p ∧ q → r) → p → q → r')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a : p → q → r', 'a_1_left : p', 'a_1_right : q'], conclusion='q')\n",
      " Tactic applied: apply a_1_right\n",
      " New Goals:  []\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop'], conclusion='(p ∧ q → r) → p → q → r')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop'], conclusion='(p ∧ q → r) → p → q → r')\n",
      " Tactic applied: intro\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r'], conclusion='p → q → r')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r'], conclusion='p → q → r')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r'], conclusion='p → q → r')\n",
      " Tactic applied: intro\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p'], conclusion='q → r')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p'], conclusion='q → r')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p'], conclusion='q → r')\n",
      " Tactic applied: intro\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p', 'a_2 : q'], conclusion='r')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p', 'a_2 : q'], conclusion='r')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p', 'a_2 : q'], conclusion='r')\n",
      " Tactic applied: apply a\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p', 'a_2 : q'], conclusion='p ∧ q')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p', 'a_2 : q'], conclusion='p ∧ q')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p', 'a_2 : q'], conclusion='p ∧ q')\n",
      " Tactic applied: split\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p', 'a_2 : q'], conclusion='p'), LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p', 'a_2 : q'], conclusion='q')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p', 'a_2 : q'], conclusion='p'), LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p', 'a_2 : q'], conclusion='q')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p', 'a_2 : q'], conclusion='p')\n",
      " Tactic applied: apply a_1\n",
      " New Goals:  []\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p', 'a_2 : q'], conclusion='q')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a : p ∧ q → r', 'a_1 : p', 'a_2 : q'], conclusion='q')\n",
      " Tactic applied: apply a_2\n",
      " New Goals:  []\n",
      " All Goals:  []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example(\n",
    "    goal = LeanGoal(['p q r : Prop'], '(p -> (q -> r)) <-> (p /\\ q -> r)'), \n",
    "    tactics=['apply {}', 'cases {}', 'intro', 'split', 'left', 'right']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal: LeanGoal(assumptions=['p q r : Prop'], conclusion='¬(p ↔ ¬p)')\n",
      "Tactics: ['apply {}', 'cases {}', 'intro', 'split', 'left', 'right']\n",
      "\n",
      "Time to proof: 32.23691010475159 seconds\n",
      "Proof: intro, cases a, apply a_mp, { apply a_mpr, intro, apply a_mp, { apply a, }, apply a, }, apply a_mpr, intro, apply a_mp, { apply a, }, apply a\n",
      "\n",
      "Verified: True\n",
      "\n",
      "Walk through proof:\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop'], conclusion='¬(p ↔ ¬p)')\n",
      " Tactic applied: intro\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ↔ ¬p'], conclusion='false')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a : p ↔ ¬p'], conclusion='false')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a : p ↔ ¬p'], conclusion='false')\n",
      " Tactic applied: cases a\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='false')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='false')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='false')\n",
      " Tactic applied: apply a_mp\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='p'), LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='p')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='p'), LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='p')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='p')\n",
      " Tactic applied: apply a_mpr\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='¬p')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='¬p'), LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='p')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='¬p')\n",
      " Tactic applied: intro\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='false')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='false'), LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='p')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='false')\n",
      " Tactic applied: apply a_mp\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p'), LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p'), LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p'), LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='p')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p')\n",
      " Tactic applied: apply a\n",
      " New Goals:  []\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p'), LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='p')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p')\n",
      " Tactic applied: apply a\n",
      " New Goals:  []\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='p')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='p')\n",
      " Tactic applied: apply a_mpr\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='¬p')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='¬p')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p'], conclusion='¬p')\n",
      " Tactic applied: intro\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='false')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='false')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='false')\n",
      " Tactic applied: apply a_mp\n",
      " New Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p'), LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p')]\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p'), LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p')\n",
      " Tactic applied: apply a\n",
      " New Goals:  []\n",
      " All Goals:  [LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p')]\n",
      "\n",
      " Primary Goal:   LeanGoal(assumptions=['p q r : Prop', 'a_mp : p → ¬p', 'a_mpr : ¬p → p', 'a : p'], conclusion='p')\n",
      " Tactic applied: apply a\n",
      " New Goals:  []\n",
      " All Goals:  []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example(\n",
    "    goal = LeanGoal(['p q r : Prop'], '¬(p ↔ ¬p)'), \n",
    "    tactics=['apply {}', 'cases {}', 'intro', 'split', 'left', 'right']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom parsing of Lean expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `apply_tactic` will return a list of goals, parsed like `LeanGoal(['m n : nat'], \"m + n = n + m\")`.  This is easy to read by a human and easy to input back into Lean, but it isn't very machine parsable and moreover, it hides a lot of information.  It is however possible to have `apply_tactic` output other information about the goals.  \n",
    "\n",
    "For example, here we right a simple piece of code which gets all the goals (both target and local context), fetches the types of the local context and prints them using a slightly more verbose and informative format which is still, nonetheless, valid Lean code.\n",
    "\n",
    "With more work, we could reformat this however we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_custom_parser = r\"\"\"\n",
    "meta def goal_to_string (g : expr) : tactic string :=\n",
    "do \n",
    "  tactic.set_goals [g],\n",
    "  goal ← tactic.target,\n",
    "  local_cxt ← tactic.local_context,\n",
    "  local_cxt_types ← list.mmap tactic.infer_type local_cxt,\n",
    "  let s := \"Goal: \" ++ (to_string goal) ++ \"\\nLocal Context: \"++ (to_string local_cxt) ++ \"\\nLocal Context Types: \" ++ (to_string local_cxt_types) ++ \"\\n\",\n",
    "  return s\n",
    "\n",
    "meta def custom_parse_state : tactic string :=\n",
    "do \n",
    " gs ← tactic.get_goals,\n",
    " -- loop over all goals (has effect of resetting the goal each time)\n",
    " goal_strings ← gs.mmap goal_to_string,\n",
    " tactic.set_goals gs,  -- set goals back\n",
    " let s := (to_string goal_strings),\n",
    " return s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Goal: eq.{1} nat (has_add.add.{0} nat nat.has_add m nat.zero) (has_add.add.{0} nat nat.has_add nat.zero m)\n",
      "Local Context: [m]\n",
      "Local Context Types: [nat]\n",
      ", Goal: eq.{1} nat (has_add.add.{0} nat nat.has_add m (nat.succ n_n)) (has_add.add.{0} nat nat.has_add (nat.succ n_n) m)\n",
      "Local Context: [m, n_n, n_ih]\n",
      "Local Context Types: [nat, nat, eq.{1} nat (has_add.add.{0} nat nat.has_add m n_n) (has_add.add.{0} nat nat.has_add n_n m)]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "with LeanTacticTestingInterface() as lean_interface:\n",
    "    lean_interface.custom_parser = simple_custom_parser\n",
    "    _, _, custom_msg = (lean_interface.apply_tactic(LeanGoal(['m n : nat'], \"m + n = n + m\"), \"induction n\"))\n",
    "print(custom_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this too what Lean prints.  This is the same information (except we don't print the goal name).\n",
    "```\n",
    "2 goals\n",
    "case nat.zero\n",
    "m : ℕ\n",
    "⊢ m + 0 = 0 + m\n",
    " \n",
    "case nat.succ\n",
    "m n_n : ℕ,\n",
    "n_ih : m + n_n = n_n + m\n",
    "⊢ m + nat.succ n_n = nat.succ n_n + m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our print out is still Lean code, which is both good and bad. It is good since it can be inputted back into Lean.  It however doesn't fully capture the abstract syntax tree (AST) of the expressions involved.  For that, it would be good to write code which serializes the AST of each expression into say s-expressions, JSON, or something else.  \n",
    "\n",
    "As an example, Lewis and Wu wrote an [interface between Mathematica and Lean](https://robertylewis.com/leanmm/), and they parse Lean expressions into Mathematica code as follows.  This is basically the same as s-expressions, and can be easily modified to return such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lean_to_mathematica_parser = r\"\"\"\n",
    "meta def form_of_name : name → string\n",
    "| name.anonymous         := \"LeanNameAnonymous\"\n",
    "| (name.mk_string s nm)  := \"LeanNameMkString[\\\"\" ++ s ++ \"\\\", \" ++ form_of_name nm ++ \"]\"\n",
    "| (name.mk_numeral i nm) := \"LeanNameMkNum[\" ++ to_string i ++ \", \" ++ form_of_name nm ++\"]\"\n",
    "\n",
    "meta def form_of_lvl : level → string\n",
    "| level.zero         := \"LeanZeroLevel\"\n",
    "| (level.succ l)     := \"LeanLevelSucc[\" ++ form_of_lvl l ++ \"]\"\n",
    "| (level.max l1 l2)  := \"LeanLevelMax[\" ++ form_of_lvl l1 ++ \",\" ++ form_of_lvl l2 ++ \"]\"\n",
    "| (level.imax l1 l2) := \"LeanLevelIMax[\" ++ form_of_lvl l1 ++ \",\" ++ form_of_lvl l2 ++ \"]\"\n",
    "| (level.param nm)   := \"LeanLevelParam[\" ++ form_of_name nm ++ \"]\"\n",
    "| (level.mvar nm)    := \"LeanLevelMeta[\" ++ form_of_name nm ++ \"]\"\n",
    "\n",
    "meta def form_of_lvl_list : list level → string\n",
    "| []       := \"LeanLevelListNil\"\n",
    "| (h :: t) := \"LeanLevelListCons[\" ++ form_of_lvl h ++ \", \" ++ form_of_lvl_list t ++\"]\"\n",
    "\n",
    "meta def form_of_binder_info : binder_info → string\n",
    "| binder_info.default             := \"BinderInfoDefault\"\n",
    "| binder_info.implicit            := \"BinderInfoImplicit\"\n",
    "| binder_info.strict_implicit     := \"BinderInfoStrictImplicit\"\n",
    "| binder_info.inst_implicit       := \"BinderInfoInstImplicit\"\n",
    "| other                           := \"BinderInfoOther\"\n",
    "\n",
    "meta def form_of_expr : expr → string\n",
    "| (expr.var i)                     := \"LeanVar[\" ++ (format.to_string (to_fmt i) options.mk)++ \"]\"\n",
    "| (expr.sort lvl)                  := \"LeanSort[\" ++ form_of_lvl lvl ++ \"]\"\n",
    "| (expr.const nm lvls)             := \"LeanConst[\" ++ form_of_name nm ++ \", \"++ form_of_lvl_list lvls ++ \"]\"\n",
    "| (expr.mvar nm nm' tp)            := \"LeanMetaVar[\" ++ form_of_name nm ++ \", \"++ form_of_expr tp ++ \"]\"\n",
    "| (expr.local_const nm ppnm bi tp) := \"LeanLocal[\" ++ form_of_name nm ++ \", \" ++form_of_name ppnm ++ \", \" ++ form_of_binder_info bi++ \", \" ++ form_of_expr tp ++ \"]\"\n",
    "| (expr.app f e)                   := \"LeanApp[\" ++ form_of_expr f ++ \", \" ++ form_of_expr e++ \"]\"\n",
    "| (expr.lam nm bi tp bod)          := \"LeanLambda[\" ++ form_of_name nm ++ \", \"++ form_of_binder_info bi ++ \", \"++ form_of_expr tp ++ \", \" ++ form_of_expr bod ++ \"]\"\n",
    "| (expr.pi nm bi tp bod)           := \"LeanPi[\" ++ form_of_name nm ++ \", \"++ form_of_binder_info bi ++ \", \" ++ form_of_expr tp++ \", \" ++ form_of_expr bod ++ \"]\"\n",
    "| (expr.elet nm tp val bod)        := \"LeanElet\"\n",
    "| (expr.macro mdf mlst)            := \"LeanMacro\"\n",
    "\n",
    "meta def custom_parse_state : tactic string :=\n",
    "do \n",
    " goal ← tactic.target,\n",
    " return (form_of_expr goal)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeanApp[LeanApp[LeanApp[LeanConst[LeanNameMkString[\"eq\", LeanNameAnonymous], LeanLevelListCons[LeanLevelSucc[LeanZeroLevel], LeanLevelListNil]], LeanConst[LeanNameMkString[\"nat\", LeanNameAnonymous], LeanLevelListNil]], LeanApp[LeanApp[LeanApp[LeanApp[LeanConst[LeanNameMkString[\"add\", LeanNameMkString[\"has_add\", LeanNameAnonymous]], LeanLevelListCons[LeanZeroLevel, LeanLevelListNil]], LeanConst[LeanNameMkString[\"nat\", LeanNameAnonymous], LeanLevelListNil]], LeanConst[LeanNameMkString[\"has_add\", LeanNameMkString[\"nat\", LeanNameAnonymous]], LeanLevelListNil]], LeanLocal[LeanNameMkNum[0, LeanNameMkNum[12, LeanNameMkString[\"_fresh\", LeanNameMkNum[0, LeanNameAnonymous]]]], LeanNameMkString[\"m\", LeanNameAnonymous], BinderInfoDefault, LeanConst[LeanNameMkNum[1, LeanNameAnonymous], LeanLevelListNil]]], LeanLocal[LeanNameMkNum[1, LeanNameMkNum[12, LeanNameMkString[\"_fresh\", LeanNameMkNum[0, LeanNameAnonymous]]]], LeanNameMkString[\"n\", LeanNameAnonymous], BinderInfoDefault, LeanConst[LeanNameMkNum[1, LeanNameAnonymous], LeanLevelListNil]]]], LeanApp[LeanApp[LeanApp[LeanApp[LeanConst[LeanNameMkString[\"add\", LeanNameMkString[\"has_add\", LeanNameAnonymous]], LeanLevelListCons[LeanZeroLevel, LeanLevelListNil]], LeanConst[LeanNameMkString[\"nat\", LeanNameAnonymous], LeanLevelListNil]], LeanConst[LeanNameMkString[\"has_add\", LeanNameMkString[\"nat\", LeanNameAnonymous]], LeanLevelListNil]], LeanLocal[LeanNameMkNum[1, LeanNameMkNum[12, LeanNameMkString[\"_fresh\", LeanNameMkNum[0, LeanNameAnonymous]]]], LeanNameMkString[\"n\", LeanNameAnonymous], BinderInfoDefault, LeanConst[LeanNameMkNum[1, LeanNameAnonymous], LeanLevelListNil]]], LeanLocal[LeanNameMkNum[0, LeanNameMkNum[12, LeanNameMkString[\"_fresh\", LeanNameMkNum[0, LeanNameAnonymous]]]], LeanNameMkString[\"m\", LeanNameAnonymous], BinderInfoDefault, LeanConst[LeanNameMkNum[1, LeanNameAnonymous], LeanLevelListNil]]]]\n"
     ]
    }
   ],
   "source": [
    "with LeanTacticTestingInterface() as lean_interface:\n",
    "    lean_interface.custom_parser = lean_to_mathematica_parser\n",
    "    _, _, custom_msg = (lean_interface.apply_tactic(LeanGoal(['m n : nat'], \"m + n = n + m\"), \"skip\"))\n",
    "print(custom_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output is effectively just the AST for the expression `m + n = n + m`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code:\n",
    "    - Is there a way to make this faster?\n",
    "    - Is there a way to make this more robust?  (It works for now, but still feels rickety.)\n",
    "    - Make this production level code.  It is just a proof of concept and still quite hacky.\n",
    "- Functionality\n",
    "    - How should one enter definitions and notation?\n",
    "    - Can we even enter all Lean theorems this way (assuming there definitions and notation are stored in mathlib)?\n",
    "      - A next step would be to get a list of all theorems and see if we can apply the `skip` tactic to them.\n",
    "    - How should one enter premises?  The easiest way is to add them as assumptions to the goal.\n",
    "    - What about imports, etc?\n",
    "    - We only apply tactics to a single goal and not a goal stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I want from Lean 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Either a REPL like interface which is fast,\n",
    "- or a way to quickly in a few milliseconds call Lean for the text of a small file and get a direct (and unambiguous) response to my request.\n",
    "- Better documentation of the communication interface\n",
    "- Better documentation of the tactic commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
